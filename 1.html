<!doctype html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <title>CS 294: Fairness in Machine Learning</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/simple.css" id="theme">
  </head>

  <body>
    <div class="reveal">
      <div class="slides">

<section>
  <h1 style="font-size:60pt;">CS 294: Fairness in Machine Learning</h1>
  <h2>Day 1: Overview</h2>
  <h2 style="color:#444">Moritz Hardt</h2>
  <!-- h3 style="color:#aaa">UC Berkeley</h3 -->
  <img style="margin:-50px 0 0 100px;width:450px;" class="plain" src="assets/berkeleylogo.png" />
  <p style="color:#888">August 25, 2017</p>
</section>

<section>
  <h2>Logistics</h2>
    <p class="fragment">This is a CS <em>grad</em> seminar. Prereqs: Stats, ML, probability,
linear algebra</p>
    <p class="fragment">Web site: <a
href="https://fairmlclass.github.io">fairmlclass.github.io</a></p>
    <p class="fragment">
    Online dicussions on Slack:
    <ul>
      <li class="fragment"> <strong>TODO:</strong> Email me for slack access link. </li>
      <li class="fragment"> You may join anonymously (won't affect grade). </li>
    </ul>
    </p>
    <p class="fragment">
    Grade: 50% in-class participation, 50% project
    <p>
    <p class="fragment">
    Not yet enrolled? Talk to me after class today.
    </p>
</section>

<section>
<div style="margin-top:100px;" />
<img class="plain" style="width:800px" src="assets/citcount-1.svg" />
</section>

<section>
<div style="margin-top:100px;" />
<img class="plain" style="width:800px" src="assets/citcount-2.svg" />
</section>

<section>
<div style="margin-top:100px;" />
<img class="plain" style="width:800px" src="assets/citcount-3.svg" />
</section>

<section>
  <h2>Rough course outline</h2>
    <p class="fragment">Part I: Sources of unfairness</p>
    <p class="fragment">Part II: Observational fairness criteria</p>
    <p class="fragment">Part III: Beyond observational fairness criteria</p>
    <p class="fragment">Part IV: Measurement and sampling</p>
    <p class="fragment">Part V: Legal and policy perspectives</p>
</section>

<section>
  <h2>Goal for this class</h2>
    <blockquote class="fragment">Develop a better understanding of a complex social
problem that will allow us to contribute to a meaningful technical discussion.</blockquote>
</section>

<section>
  <h2>The dangers of math snobbery</h2>
  <ul>
  <li class="fragment">Technical work without understanding social context.</li>
  <li class="fragment">Thinking we're more rigorous than social scientists.</li>
  <li class="fragment">Justifying an approach by the math it entails.</li>
  </ul>
</section>

<section>
  <h2>Sanity checks for taking this class</h2>
  <p class="fragment">You don't mind reading social science papers</p>
  <p class="fragment">You'd still be here even if the chance of getting a paper
out of it is 0.</p>
</section>

<section>
  <h2>Part I: Sources of unfairness</h2>
  <p class="fragment" style="margin-top:15%;">How can machine learning wind up<br>
  being <em>unfair</em> without any explicit wrongdoing?</p>
</section>

<section>
  <h2>The sample size disparity</h2>
  <div style="width:40%;background-color:#fff;" class="left">
    <img class="plain" width="100%" src="assets/error_rate.svg" />
  </div>
  <div style="width:58%;" class="right">
    <p>&nbsp;</p>
    <p class="fragment">Generally, more data means smaller error</p>
    <p class="fragment">By definition, less data on minority groups.</p>
    <p class="fragment">Can lead to higher error rates on minority.</p>
  </div>
</section>

<section>
  <h2>The meaning of <em>low error</em></h2>
  <p>Two classifiers with 5% average error:</p>
  <p><img width="60%;" class="plain" src="assets/random_systematic.svg" />
</section>

<section>
  <h2>Biases in data</h2>
  <p class="fragment"><img class="plain" src="assets/oxymoron.jpg" />
</section>
<section>
  <h2>Biases in data</h2>
  <p class="fragment">Collection: 
    <ul>
     <li class="fragment">Demographic, geographic, behavioral, temporal biases, </li>
    </ul>
  </p>
  <p class="fragment">Measurement:
     <ul>
       <li class="fragment">What do we choose to measure? How do we measure
(e.g., <em>grit</em>)?</li>
     </ul>
  </p>
  <p class="fragment">Pre-existing biases
  <ul>
  <li class="fragment">gender roles in text and images, racial stereotypes in
language</li>
  </ul>
  </p>
</section>

<section>
 <h2>ML models reflect underlying data</h2>
 <img class="plain" style="" src="assets/science-semantics.png" />
</section>

<section>
 <h2>ML models reflect underlying data</h2>
 <img class="plain" style="" src="assets/word-embedding-bias.png" />
</section>

<section>
  <h2>A sequence of White House reports</h2>
 <img class="plain" style="" src="assets/big-data-white-house.png" />
</section>


<section>
  <h2>On Monday</h2>
  <p>We'll discuss:</p>
    <blockquote>Barocas, Selbst. Big Data's Disparate Impact</blockquote>
  <p>Before we meet:</p>
  <p>Skim the whole thing. Choose one part to read very carefully.</p>
</section>

<section>
  <h2>Background reading</h2>
<ul>
<li>Boyd and Crawford, “Critical Questions for Big Data”</li>
<li>O'Neill, Weapons of Math Destruction</li>
<li>Pasquale, The Black Box Society</li>
<li>The White House Office of Science and Technology Policy, Big Data: A Report
on Algorithmic Systems, Opportunity, and Civil Rights</li>
</ul>
</section>

<section>
<h2>Quick Outlook</h2>
</section>

<section>
<h2>Part II:<br />Observational fairness criteria</h2>
<p class="fragment">Many definitions</p>
<p class="fragment">Algorithms for achieving them</p>
<p class="fragment">Trade-offs</p>
<p class="fragment">Impossibility results</p>
</section>

<section>
<h2>Typical setup</h2>
<p class="fragment">$X$ features of an individual</p>
<p class="fragment">$A$ sensitive attribute (race, gender, ...)</p>
<p class="fragment">$C=C(X,A)$ classifier mapping $X$ and $A$ to some prediction</p>
<p class="fragment">$Y$ actual outcome</p>
<p class="fragment">Note: random variables in the same probability space</p>
</section>

<section>
<h2>All of this is a lie</h2>
<p class="fragment">$X$ incorporates all sorts of measurement biases</p>
<p class="fragment">$A$ often not even known, ill-defined, misreported, inferred</p>
<p class="fragment">$C$ often not well defined, e.g., large production ML system</p>
<p class="fragment">$Y$ often poor proxy of actual variable of interest</p>
</section>

<section>
<h2>Demographic parity</h2>
<p class="fragment">
Assume $C$ and $A$ are binary $0/1$-variables.
</p>
<p class="fragment"><strong>Definition.</strong>
Classifier $C$ satisfies <em>demographic parity</em> if<br />
$\mathbb{P}\{ C = 1 \mid A = 1 \} = \mathbb{P}\{ C = 1 \mid A = 0 \}$.
</p>
</section>

<section>
<h2>Accuracy parity</h2>
<p class="fragment">
Assume $A$ is binary $0/1$-variable.
</p>
<p class="fragment"><strong>Definition.</strong>
Classifier $C$ satisfies <em>accuracy parity</em> if<br />
$\mathbb{P}\{ C = Y \mid A = 1 \} = \mathbb{P}\{ C = Y \mid A = 0 \}$.
</p>
</section>

<section>
<h2>Precision parity</h2>
<p class="fragment">
Assume $C$, $Y$ and $A$ are binary $0/1$-variables.
</p>
<p class="fragment"><strong>Definition.</strong>
Classifier $C$ satisfies <em>precision parity</em> if<br />
$\mathbb{P}\{ Y = 1 \mid C=1, A = 1 \} = \mathbb{P}\{ Y = 1\mid C=1, A = 0 \}$.
</p>
</section>

<section>
<h2>True positive parity</h2>
<p class="fragment">
Assume $C$, $Y$ and $A$ are binary $0/1$-variables.
</p>
<p class="fragment"><strong>Definition.</strong>
Classifier $C$ satisfies <em>true positive parity</em> if<br />
$\mathbb{P}\{ C = 1 \mid Y=1, A = 1 \} = \mathbb{P}\{ C = 1\mid Y=1, A = 0 \}$.
</p>
</section>

<section>
<h2>Observational criteria</h2>
<p class="fragment"><strong>Definition.</strong> A criterion is called <em>observational</em> if
it is a property of the joint distribution of features $X,A$, classifier $C$,
and outcome $Y$.
</p>
<p>&nbsp;</p>
<p class="fragment">Examples: 
Everything we just saw, and many others.
</p>
</section>

<section>
<h2>Questions we'll work on</h2>
<p class="fragment">What can we learn from observational criteria?</p>
<p class="fragment">How can we achieve them algorithmically?</p>
<p class="fragment">How do they trade-off?</p>
<p class="fragment">How do these criteria shape public discourses?</p>
<p class="fragment">Key example: COMPAS debate on crime recidivism risk scores</p>
</section>

<section>
<h2>COMPAS: An observational debate</h2>
<p>
<img class="plain" src="assets/machine-bias.png" />
</p>
</section>

<section>
<h2>COMPAS: An observational debate</h2>
<p>Probublica's main charge was observational.</p>
<blockquote class="fragment">Black defendants experienced higher false positive
rate.</blockquote>
<p class="fragment">Northpointe's main defense was observational.</p>
<blockquote class="fragment">
Scores satisfy precision parity.
</blockquote>
</section>

<section>
<h2>Trade-offs are necessary</h2>
<p style="text-align:left">
A classifier $C$ cannot simultaneously achieve (a) precision parity, (b) true
positive parity, and (c) false positive parity unless:
<ul>
<li class="fragment">$C=Y$ (the classifier is perfect), <em>or</em></li>
<li class="fragment">$\mathbb{P}\{Y=1\mid A=0\}=\mathbb{P}\{Y=1\mid A=1\}$ (base rates are equal)</li>
</ul>
</p>
<p class="fragment">Due to Kleinberg, Mullainathan, Raghavan (2016), and Chouldechova (2016),
although stated somewhat differently.</p>
</section>

<section>
  <h2 style="font-size:58pt;letter-spacing:-2pt;">Observational criteria have inherent limitations</h2>
  <p class="fragment">There are two scenarios with identical joint distributions,<br /> 
but completely different interpretations for fairness.</p>
  <p class="fragment">In particular, no observational definition<br /> can distinguish the two scenarios.</p>
  <p class="fragment">Due to H, Price, Srebro (2016)</p>
</section>


<section>
<h2>Part III:<br />
Beyond Observational Measures</h2>
<p class="fragment">Causality</p>
<p class="fragment">Deep dive into causal graphs, causal inference,
interventions, matching</p>
<p class="fragment">Develop causal fairness criteria (definitions, algorithms,
trade-offs, ...) </p>
<p class="fragment">Relationship to similarity-based fairness notions
("individual fairness")
</section>

<section>
<h2>Background reading</h2>
<img class="plain" src="assets/pearl-causality.jpeg" />
</section>

<section>
<h2>Part IV:<br />
Measurement and sampling
</h2>
<p class="fragment">Measurement theory, sampling theory</p>
<p class="fragment">Developing awareness of pitfalls</p>
<p class="fragment">Understand data-generating processes better</p>
</section>
<section>
<h2>Part V:<br />
Legal and policy perspectives
</h2>
<p class="fragment">Understand legal challenges technical work faces</p>
<p class="fragment">Think through possibility of policy recommendations</p>
</section>
<section>
<div style="margin-top:400px;">
<h2>Thank you.</h2>
</div>
</section>

      </div>
    </div>

    <script src="js/head.min.js"></script>
    <script src="js/reveal.js"></script>

    <script>
      Reveal.initialize({
        history: true,
        transition: 'none',
        height:900, //"100%",
        width:1600, //"100%",
        // Factor of the display size that should remain empty around the content
        margin: 0.05,
        // Bounds for smallest/largest possible scale to apply to content
        minScale: 0.2,
        maxScale: 1.5,
        center: false,
        math: {
          // mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full'
        },
        dependencies: [
          { src: 'js/classList.js' },
          { src: 'js/math.js', async: true }
        ]
      });
    </script>
  </body>
</html>
